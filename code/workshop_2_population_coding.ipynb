{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655252d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Header: introduce dataset and goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11244721",
   "metadata": {},
   "source": [
    "![Image](./resources/cropped-SummerWorkshop_Header.png)\n",
    "\n",
    "<h1 align=\"center\">Population Coding</h1> \n",
    "<h2 align=\"center\"> Day 2, Afternoon Session</h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f95c14",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "In the first workshop of today, we examined how sensory variables can be encoded in individual neurons' activity. We now turn our attention to the coordinated activity of groups of neurons: population codes!\n",
    "    \n",
    "### How do populations of neurons encode information about sensory stimuli? \n",
    "### How are these population codes modulated by context or behavioral state? \n",
    "### What other types of thing are encoded in population activity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6e6e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replace this section with the standard setup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from allensdk.brain_observatory.behavior.behavior_project_cache.\\\n",
    "    behavior_neuropixels_project_cache \\\n",
    "    import VisualBehaviorNeuropixelsProjectCache\n",
    "\n",
    "base_dir = \"/Users/gkocker/Documents/projects/swdb_2024_physiology\"\n",
    "data_dir = os.path.join(base_dir, \"data\")\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = VisualBehaviorNeuropixelsProjectCache.from_s3_cache(\n",
    "            cache_dir=Path(data_dir))\n",
    "\n",
    "# get the metadata tables\n",
    "units_table = cache.get_unit_table()\n",
    "\n",
    "channels_table = cache.get_channel_table()\n",
    "\n",
    "probes_table = cache.get_probe_table()\n",
    "\n",
    "behavior_sessions_table = cache.get_behavior_session_table()\n",
    "\n",
    "ecephys_sessions_table = cache.get_ecephys_session_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e6aa9",
   "metadata": {},
   "source": [
    "Grab data from a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = cache.get_ecephys_session(\n",
    "           ecephys_session_id=1065437523)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b28561c",
   "metadata": {},
   "source": [
    "The stimulus presentations table is a record of every stimulus we presented to the mouse over the course of this experiment. Let's take a look at this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4914fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_presentations = session.stimulus_presentations\n",
    "stimulus_presentations.head(-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab132b56",
   "metadata": {},
   "source": [
    "It contains a great deal of information about the stimulus trials! Let's look at all the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f35cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_presentations.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff59a6f0",
   "metadata": {},
   "source": [
    "The different stimuli are indexed by the 'stimulus_block' column. Let's group this dataframe by stimulus block and see what stimulus was shown for each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e670b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_presentations.groupby('stimulus_block')[['stimulus_block', \n",
    "                                                'stimulus_name', \n",
    "                                                'active', \n",
    "                                                'duration', \n",
    "                                                'start_time']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc20e0",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 10px; padding-left: 10px; padding-bottom: 10px; background: #c8e0bf; \">\n",
    "    \n",
    "What are the types of stimuli that were presented?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30095ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f7200e0",
   "metadata": {},
   "source": [
    "Now let's get unit and channel data, sort the units by depth and filter for \"good\" units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get unit and channel data, sort the units by depth and filter for \"good\" units\n",
    "units = session.get_units()\n",
    "channels = session.get_channels()\n",
    "\n",
    "unit_channels = units.merge(channels, left_on='peak_channel_id', right_index=True)\n",
    "\n",
    "#first let's sort our units by depth and filter\n",
    "unit_channels = unit_channels.sort_values('probe_vertical_position', ascending=False)\n",
    "\n",
    "#now we'll filter them\n",
    "good_unit_filter = ((unit_channels['snr']>1)&\n",
    "                    (unit_channels['isi_violations']<1)&\n",
    "                    (unit_channels['firing_rate']>0.1))\n",
    "\n",
    "good_units = unit_channels.loc[good_unit_filter]\n",
    "spike_times = session.spike_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7953bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_units.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd98a46",
   "metadata": {},
   "source": [
    "Which brain structures were recorded from in this session?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57950288",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_channels.value_counts('structure_acronym')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddd114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's look at population activity in an area of interest\n",
    "area_of_interest = 'VISp'\n",
    "area_units = good_units[good_units['structure_acronym'] == area_of_interest]\n",
    "num_units = len(area_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8927f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_presentations = session.stimulus_presentations\n",
    "stimulus_presentations = stimulus_presentations[stimulus_presentations.stimulus_name == 'Natural_Images_Lum_Matched_set_ophys_G_2019']\n",
    "stimulus_presentations.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84114fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### What stimuli are presented in these blocks?\n",
    "np.sort(stimulus_presentations['image_name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de43de",
   "metadata": {},
   "source": [
    "### Let's start by looking at the neural activity! Does it reflect the image presentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot a single-trial raster, population PSTH, and representation matrix\n",
    "pre_time = 1\n",
    "post_time = 1\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "presentation_idx = 1\n",
    "start_time = stimulus_presentations['start_time'][presentation_idx] \n",
    "end_time = stimulus_presentations['end_time'][presentation_idx]\n",
    "\n",
    "unit_num = 0\n",
    "for iu, unit in area_units.iterrows():\n",
    "    unit_spike_times = spike_times[iu]\n",
    "    \n",
    "    unit_spike_times = unit_spike_times[(unit_spike_times >= start_time - pre_time) * (unit_spike_times < end_time + post_time)]\n",
    "    unit_num_spikes = len(unit_spike_times)\n",
    "    \n",
    "    plt.plot(unit_spike_times - start_time, unit_num*np.ones(unit_num_spikes,), 'k|', markersize=5)\n",
    "    unit_num += 1\n",
    "\n",
    "plt.xlabel('Time relative to stimulus presentation (s)')\n",
    "plt.ylabel('Unit')\n",
    "plt.ylim((0, num_units+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5917f8",
   "metadata": {},
   "source": [
    "### Let's compare to a change trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62007f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### plot a single-trial raster, population PSTH, and representation matrix\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "change_idx = np.where(stimulus_presentations['is_change'].values)[0]\n",
    "presentation_idx = change_idx[0]\n",
    "\n",
    "start_time = stimulus_presentations['start_time'][presentation_idx]\n",
    "end_time = stimulus_presentations['end_time'][presentation_idx]\n",
    "\n",
    "unit_num = 0\n",
    "for iu, unit in area_units.iterrows():\n",
    "    unit_spike_times = spike_times[iu]\n",
    "    \n",
    "    unit_spike_times = unit_spike_times[(unit_spike_times >= start_time - pre_time) * (unit_spike_times < end_time + post_time)]\n",
    "    unit_num_spikes = len(unit_spike_times)\n",
    "    \n",
    "    plt.plot(unit_spike_times - start_time, unit_num*np.ones(unit_num_spikes,), 'k|', markersize=5)\n",
    "    unit_num += 1\n",
    "\n",
    "plt.xlabel('Time relative to stimulus presentation (s)')\n",
    "plt.ylabel('Unit')\n",
    "plt.ylim((0, num_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024b4d7",
   "metadata": {},
   "source": [
    "### Now let's take a look at the trial-averaged responses to see how a neuron encodes the stimulus in its time-dependent firing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convenience function to compute the PSTH\n",
    "def makePSTH(spikes, startTimes, windowDur, binSize=0.001):\n",
    "    bins = np.arange(0,windowDur+binSize,binSize)\n",
    "    counts = np.zeros(bins.size-1)\n",
    "    for i,start in enumerate(startTimes):\n",
    "        startInd = np.searchsorted(spikes, start)\n",
    "        endInd = np.searchsorted(spikes, start+windowDur)\n",
    "        counts = counts + np.histogram(spikes[startInd:endInd]-start, bins)[0]\n",
    "    \n",
    "    counts = counts/startTimes.size\n",
    "    return counts/binSize, bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a311e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot a set of PSTHs\n",
    "presentations_sorted = stimulus_presentations.sort_values(by='image_name')\n",
    "num_units = len(area_units)\n",
    "\n",
    "stimuli = presentations_sorted['image_name'].unique()\n",
    "good_stim = [p is not np.nan for p in stimuli]\n",
    "stimuli = stimuli[good_stim]\n",
    "num_stim = len(stimuli)\n",
    "\n",
    "tuning_curves = np.zeros((num_units, num_stim))\n",
    "unit_num = 0\n",
    "\n",
    "stimulus = stimuli[0]\n",
    "\n",
    "psths = []\n",
    "time_before_im = 1\n",
    "duration = 2\n",
    "\n",
    "for iu, unit in area_units.iterrows():\n",
    "    unit_spike_times = spike_times[iu]\n",
    "    \n",
    "    presentations = presentations_sorted[presentations_sorted['image_name'] == stimulus]\n",
    "    num_presentations = len(presentations)\n",
    "    \n",
    "    start_times = presentations['start_time'].values\n",
    "    \n",
    "    unit_response, bins = makePSTH(unit_spike_times, \n",
    "                                      start_times - time_before_im, \n",
    "                                      duration, binSize=0.01)\n",
    "    \n",
    "    psths.append(unit_response)\n",
    "\n",
    "psths = np.array(psths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd750cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(bins[:-1] - time_before_im, psths.T);\n",
    "plt.xlabel('Time from change (s)')\n",
    "plt.ylabel('Firing rate (Hz)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d524f9bf",
   "metadata": {},
   "source": [
    "We can see the trial structure of the task reflected in the PSTH!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091dd7ef",
   "metadata": {},
   "source": [
    "## Training a classifier on population spiking data\n",
    "\n",
    "In order to determine how well we can decode the stimulus direction from population activity, we will train a **classifier** on our matrix of firing rates. Whereas regression models try to predict continuous values from the input features, classification models try to predict *labels* (also known as classes) from the input features.\n",
    "\n",
    "### Support Vector Machines\n",
    "\n",
    "Let's start with a linear Support Vector Machine (SVM) classifier, which will try to draw linear boundaries between orientation conditions (the labels) in our 94-dimensional firing rate space.\n",
    "\n",
    "This cartoon shows how we would expect an SVM to behave on a much simpler dataset, which has two dimensions and three conditions:\n",
    "\n",
    "![SVM illustration](./resources/svm-classifier.png)\n",
    "\n",
    "SVM computes decision boundaries in feature space that can be used to classify different conditions. If a new data point appears, the SVM classifier will label it based on where it falls with respect to these boundaries.\n",
    "\n",
    "To train an SVM, we need to import the following methods from `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f21c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2c41c",
   "metadata": {},
   "source": [
    "### First, we need to create a response matrix and vector of stimulus labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e3d4d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stimulus_presentations = session.stimulus_presentations\n",
    "stimulus_presentations = stimulus_presentations[stimulus_presentations.stimulus_name == 'Natural_Images_Lum_Matched_set_ophys_G_2019']\n",
    "stimulus_presentations = stimulus_presentations[stimulus_presentations.active]\n",
    "\n",
    "num_presentations = len(stimulus_presentations)\n",
    "stimulus_presentations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_response_array(spike_times, stimulus_presentations, units, window=.05):\n",
    "\n",
    "    '''\n",
    "    Create an array of spike counts x stimulus presentations, and a corresponding list of stimulus label\n",
    "    spike_times: spike times \n",
    "    stimulus_presentation: stimulus presentation table\n",
    "    units: units table containing only the units to get the responses of\n",
    "    '''\n",
    "    \n",
    "    labels = []\n",
    "    responses = []\n",
    "\n",
    "    presentation_num = 0\n",
    "\n",
    "    for presentation_idx, presentation in stimulus_presentations.iterrows():    \n",
    "        start_time = stimulus_presentations['start_time'][presentation_idx]\n",
    "        end_time = stimulus_presentations['end_time'][presentation_idx]\n",
    "        duration = end_time - start_time\n",
    "\n",
    "        if np.isnan(duration):\n",
    "            continue\n",
    "            \n",
    "        if window > duration:\n",
    "            print('Warning: window size longer than stimulus presentation')\n",
    "\n",
    "        labels.append(presentation.image_name)\n",
    "\n",
    "        unit_num = 0\n",
    "\n",
    "        presentation_responses = np.zeros((num_units,))\n",
    "\n",
    "        for iu, unit in units.iterrows():\n",
    "            unit_spike_times = spike_times[iu]\n",
    "            presentation_spike_times = unit_spike_times[(unit_spike_times >= start_time) * (unit_spike_times < start_time+window)]\n",
    "\n",
    "            presentation_responses[unit_num] = len(presentation_spike_times) / duration\n",
    "\n",
    "            unit_num += 1\n",
    "\n",
    "        responses.append(presentation_responses)\n",
    "        presentation_num += 1\n",
    "\n",
    "    responses = np.array(responses)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return responses, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33612eeb",
   "metadata": {},
   "source": [
    "### Note: creating the response matrix might take a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4284c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses, labels = make_response_array(spike_times, stimulus_presentations, area_units, window=.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a10fb5",
   "metadata": {},
   "source": [
    "We will first select a random subset of trials for training the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778764ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_presentations = responses.shape[0]\n",
    "num_train = int(total_presentations * 0.5) # Use 50% of trials for training\n",
    "random_trial_order = np.random.permutation(responses.shape[0])\n",
    "train_indices = random_trial_order[:num_train]\n",
    "\n",
    "training_data = responses[train_indices]\n",
    "training_labels = labels[train_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc65166",
   "metadata": {},
   "source": [
    "Next, we'll create the model and fit it to our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2735d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(responses[train_indices], labels[train_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7ef93",
   "metadata": {},
   "source": [
    "Now that our model has been trained, we can ask it to classify unlabeled data (i.e., the sets of population firing rates that were not included in our original training set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b96247",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = random_trial_order[num_train:]\n",
    "test_data = responses[test_indices]\n",
    "predicted_labels = clf.predict(responses[test_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c260ff5d",
   "metadata": {},
   "source": [
    "We can compare the predicted labels to the actual labels in order to assess the classifier's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ad937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_int(labels, conditions):\n",
    "\n",
    "    conversion_dict = {}\n",
    "    for i, label in enumerate(conditions):\n",
    "        conversion_dict[label] = i\n",
    "\n",
    "    labels_as_int = [conversion_dict[l] for l in labels]\n",
    "\n",
    "    return labels_as_int    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe2930",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = np.unique(labels)\n",
    "\n",
    "actual_labels = labels[test_indices]\n",
    "accuracy = np.mean(actual_labels == predicted_labels)\n",
    "\n",
    "print('Accurary: {}'.format(accuracy))\n",
    "print('Chance level: {}'.format(1/len(conditions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dbbe31",
   "metadata": {},
   "source": [
    "We see that we perform better than chance, but not very well! We can get a better sense of classification performance by using the `scikit-learn.model_selection.KFold` iterator to automatically split up the data into \"train\" and \"test\" sets for 5 iterations. Note that the model is fit independently on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b75b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "confusions = []\n",
    "\n",
    "conditions = np.unique(labels)\n",
    "num_splits = 5\n",
    "\n",
    "for train_indices, test_indices in KFold(n_splits=num_splits, shuffle=True).split(responses):\n",
    "    \n",
    "    clf = svm.SVC()\n",
    "#     clf = RandomForestClassifier(min_samples_leaf=20)\n",
    "#     clf = LinearDiscriminantAnalysis()\n",
    "\n",
    "    clf.fit(responses[train_indices], labels[train_indices])\n",
    "    \n",
    "    test_targets = labels[test_indices]\n",
    "    test_predictions = clf.predict(responses[test_indices])\n",
    "    \n",
    "    accuracy = np.mean(test_targets == test_predictions)    \n",
    "    print(accuracy)\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    confusions.append(confusion_matrix(y_true=test_targets, y_pred=test_predictions, labels=conditions, normalize='pred'))\n",
    "    \n",
    "print(f\"\\nmean accuracy: {np.mean(accuracies)}\")\n",
    "print(f\"chance: {1/conditions.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319aa5c",
   "metadata": {},
   "source": [
    "The 5-fold cross-validation roughly agrees with our previous result. Are there particular stimuli that drive the errors? Do assess this we'll look at the confusion matrix, which tells us how frequently stimulus 1 is predicted when any stimulus is shown (and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d567571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusions, conditions):\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    \n",
    "    mean_confusion = np.mean(confusions, axis=0)\n",
    "\n",
    "    plt.imshow(mean_confusion)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.xticks(range(len(conditions)), conditions, rotation=45)\n",
    "    plt.yticks(range(len(conditions)), conditions)\n",
    "\n",
    "    plt.xlabel(\"predicted\")\n",
    "    plt.ylabel(\"actual\")\n",
    "    \n",
    "plot_confusion_matrix(confusions, conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a6dbb",
   "metadata": {},
   "source": [
    "What structure do you see in the confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d57453",
   "metadata": {},
   "source": [
    "## Exploring the time course of visual information "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f20680",
   "metadata": {},
   "source": [
    "\n",
    "Next we'll examine the time course of information in our population! Or more specifically: how the length of the spike count window affects the decoding accuracy. Can we decode the stimulus perfectly if we integrate spikes for long enough? First, let's try decoding with a longer response window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fed5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses, labels = make_response_array(spike_times, stimulus_presentations, area_units, window=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db150ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "confusions = []\n",
    "\n",
    "conditions = np.unique(labels)\n",
    "num_splits = 5\n",
    "\n",
    "for train_indices, test_indices in KFold(n_splits=num_splits, shuffle=True).split(responses):\n",
    "    \n",
    "#     clf = svm.SVC(gamma=\"scale\", kernel=\"rbf\")\n",
    "#     clf = RandomForestClassifier(min_samples_leaf=20)\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "\n",
    "    clf.fit(responses[train_indices], labels[train_indices])\n",
    "    \n",
    "    test_targets = labels[test_indices]\n",
    "    test_predictions = clf.predict(responses[test_indices])\n",
    "    \n",
    "    accuracy = np.mean(test_targets == test_predictions)    \n",
    "    print(accuracy)\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    confusions.append(confusion_matrix(y_true=test_targets, y_pred=test_predictions, labels=conditions, normalize='pred'))\n",
    "    \n",
    "print(f\"\\nmean accuracy: {np.mean(accuracies)}\")\n",
    "print(f\"chance: {1/conditions.size}\")\n",
    "\n",
    "plot_confusion_matrix(confusions, conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca488e2",
   "metadata": {},
   "source": [
    "With a long response window we can decode the stimulus perfectly! How long do we need to integrate spikes for to do so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbbefe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_lengths = np.arange(.01, .1, .02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168aa299",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = np.zeros((len(window_lengths), num_splits))\n",
    "\n",
    "for i, window in enumerate(window_lengths):\n",
    "    print('{}/{}'.format(i, len(window_lengths)))\n",
    "    responses, labels = make_response_array(spike_times, stimulus_presentations, area_units, window)\n",
    "    \n",
    "    k = 0\n",
    "    for train_indices, test_indices in KFold(n_splits=num_splits, shuffle=True).split(responses):\n",
    "        clf = svm.SVC()\n",
    "        clf.fit(responses[train_indices], labels[train_indices])\n",
    "\n",
    "        test_targets = labels[test_indices]\n",
    "        test_predictions = clf.predict(responses[test_indices])\n",
    "\n",
    "        accuracies[i, k] = np.mean(test_targets == test_predictions)        \n",
    "              \n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf1eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.errorbar(x=window_lengths, y=accuracies.mean(axis=(1)), yerr=accuracies.std(axis=(1)), fmt='o-')\n",
    "\n",
    "plt.xlabel('Spike counting window length (s)')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ee528c",
   "metadata": {},
   "source": [
    "## Relationship between population size and decoding accuracy\n",
    "\n",
    "Next we'll examine how the size of the simultaneously recorded population affects decoding accuracy. In any physiology experiment, we only have a very small window into the overall population response. For example, there are about 500,000 neurons in mouse V1, so in this case we are measuring around 0.02% of the firing rates in this region.\n",
    "\n",
    "As the number of simultaneously recorded neurons increases, we expect that our ability to decode stimulus identity will improve. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b538fd1",
   "metadata": {},
   "source": [
    "### To start with, let's try decoding with a random sample of 10 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b363128",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = 10\n",
    "\n",
    "pop_idx = np.random.choice(range(num_units), size=pop_size)\n",
    "responses_pop = responses[:, pop_idx]\n",
    "\n",
    "accuracies = []\n",
    "confusions = []\n",
    "\n",
    "for train_indices, test_indices in KFold(n_splits=num_splits, shuffle=True).split(responses_pop):\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(responses_pop[train_indices], labels[train_indices])\n",
    "\n",
    "    test_targets = labels[test_indices]\n",
    "    test_predictions = clf.predict(responses_pop[test_indices])\n",
    "\n",
    "    accuracy = np.mean(test_targets == test_predictions)    \n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    confusions.append(confusion_matrix(y_true=test_targets, y_pred=test_predictions, labels=conditions, normalize='pred'))\n",
    "    \n",
    "print(f\"\\nmean accuracy: {np.mean(accuracies)}\")\n",
    "print(f\"chance: {1/conditions.size}\")\n",
    "\n",
    "plot_confusion_matrix(confusions, conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f7d743",
   "metadata": {},
   "source": [
    "Does the result depend on which 10 neurons we sampled? Let's try another random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75752930",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_idx = np.random.choice(range(num_units), size=pop_size)\n",
    "responses_pop = responses[:, pop_idx]\n",
    "\n",
    "accuracies = []\n",
    "confusions = []\n",
    "\n",
    "for train_indices, test_indices in KFold(n_splits=num_splits, shuffle=True).split(responses_pop):\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(responses_pop[train_indices], labels[train_indices])\n",
    "\n",
    "    test_targets = labels[test_indices]\n",
    "    test_predictions = clf.predict(responses_pop[test_indices])\n",
    "\n",
    "    accuracy = np.mean(test_targets == test_predictions)    \n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    confusions.append(confusion_matrix(y_true=test_targets, y_pred=test_predictions, labels=conditions, normalize='pred'))\n",
    "    \n",
    "print(f\"\\nmean accuracy: {np.mean(accuracies)}\")\n",
    "print(f\"chance: {1/conditions.size}\")\n",
    "\n",
    "plot_confusion_matrix(confusions, conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a764d07c",
   "metadata": {},
   "source": [
    "### Now, let's try to get a sense for how this changes with the number of neurons we use to train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a3dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_sizes = np.arange(1, 50, 5).astype('int')\n",
    "num_resamples = 10\n",
    "\n",
    "accuracies = np.zeros((len(pop_sizes), num_resamples, num_splits))\n",
    "\n",
    "for i, pop_size in enumerate(pop_sizes):\n",
    "    print('population size: {}'.format(pop_size))\n",
    "\n",
    "    for j in range(num_resamples):\n",
    "        pop_idx = np.random.choice(range(num_units), size=pop_size)\n",
    "        responses_pop = responses[:, pop_idx]\n",
    "\n",
    "        k = 0\n",
    "        for train_indices, test_indices in KFold(n_splits=num_splits, shuffle=True).split(responses_pop):\n",
    "            clf = svm.SVC()\n",
    "            clf.fit(responses_pop[train_indices], labels[train_indices])\n",
    "\n",
    "            test_targets = labels[test_indices]\n",
    "            test_predictions = clf.predict(responses_pop[test_indices])\n",
    "\n",
    "            accuracy = np.mean(test_targets == test_predictions)    \n",
    "\n",
    "            accuracies[i, j, k] = accuracy\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.errorbar(x=pop_sizes, y=accuracies.mean(axis=(1, 2)), yerr=accuracies.std(axis=(1, 2)), fmt='o-')\n",
    "\n",
    "plt.xlabel('Population size')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5d1f1",
   "metadata": {},
   "source": [
    "Roughly how many neurons do you need to decode with 50% accuracy? 80%? 90%?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c24c50f",
   "metadata": {},
   "source": [
    "## Exploring correlations between neurons\n",
    "\n",
    "Finally, we turn to examine the structure of the population activity.\n",
    "Does the structure of the population activity matter for this decoding, or is single-neuron tuning the whole name of the game? For example, if neurons 1 and 2 are co-active on trial 1 (both above their individual mean activity), does that carry any extra information? To explore this, we'll look at correlations between their responses.\n",
    "\n",
    "<!-- Based on the plot above, it's clear that neurons are correlated with one another. For example, look at units 35-40 and notice how they tend to have high firing rates or low firing rates on similar trials. -->\n",
    "\n",
    "We'll look at this correlation in much more detail below, but we should first note some assumptions. Primarily, we are studying *spike counts*, or rates within time windows defined by the stimulus. This assumes that all spikes within the windows are equivalent, no matter their relative timing. It also assumes a specific set of time windows (set by the stimulus). In some cases, these assumptions may not be desirable (e.g., in studies of time-lagged spike-spike correlation, frequently used in studies of functional connectivity.)\n",
    "\n",
    "With that tangent aside, let's return to our observation that the neurons' activities (defined here by spike rates) are correlated.\n",
    "\n",
    "<!-- The activities of correlated neural populations have a *lower dimensionality* than the number of neurons. For example, for two perfectly correlated neurons, a single number suffices to describe both of their firing rates. This same idea applies to larger populations, and to less-than-perfect correlations. -->\n",
    "\n",
    "<!-- To explore this property, we will apply the most common dimensionality reduction technique in existence to these data: Principal Component Analysis (PCA). This is a linear dimensionality reduction method (more on this later), and it works by considering the space of all possible neuron responses, wherein each axis of the space is a single neuron's firing rate. PCA finds the directions in this space along which the activities are the most spread out (highest variance) or the least spread out. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc952d9b",
   "metadata": {},
   "source": [
    "## Computing correlation matrices\n",
    "\n",
    "So far, we have looked at the covariance between neurons. For the following analysis, we will instead look at Pearson correlations instead of covariance: the Pearson correlation for a pair of neurons is the covariance divided by the product of the neurons' standard deviations. This normalizes the measure so that its maximum is 1 and minimum is -1, which makes it easier to interpret than covariances.\n",
    "\n",
    "So far, we have not considered how much of the covariance or correlation is stimulus-driven (e.g., reflecting neurons with similar tuning responding to the same stimulus at the same time) vs arising from other sources. \n",
    "\n",
    "The correlations due to the stimulus properties are called *signal correlations*, whereas correlations due to other sources (including random variability within the eyes and the brain) are called *noise correlations*. The correlations we considered above encapsulate both of these factors, and are called *total* correlations.\n",
    "\n",
    "To separate these out, we'll now compute and compare all 3 (Pearson) correlation matrices: the total correlations, signal correlations, and noise correlations.\n",
    "\n",
    "First, the total correlations (using `np.corrcoef`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72575c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_correlations = np.corrcoef(responses.T)\n",
    "plt.imshow(total_correlations, cmap='bwr', clim=(-1,1))\n",
    "plt.colorbar(label='Correlation coefficient')\n",
    "plt.title('Total Correlations')\n",
    "plt.xlabel('Unit #')\n",
    "plt.ylabel('Unit #')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec7a26f",
   "metadata": {},
   "source": [
    "Next, we'll compute the signal correlations. These are the correlations in the neurons' average response to each stimulus (defined by orientation), computed across stimuli. As the name implies, they tell us how much two neurons' mean (trial averaged) activities co-vary as the stimulus changes.\n",
    "\n",
    "To compute these, we'll first calculate the average activities for each stimulus identify and neuron, then compute the correlation matrix across stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07cebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute tuning curves - takes a few minutes\n",
    "\n",
    "presentations_sorted = stimulus_presentations.sort_values(by='image_name')\n",
    "num_units = len(area_units)\n",
    "\n",
    "stimuli = np.sort(presentations_sorted['image_name'].unique())\n",
    "num_stim = len(stimuli)\n",
    "\n",
    "tuning_curves = np.zeros((num_units, num_stim))\n",
    "unit_num = 0\n",
    "\n",
    "durations = []\n",
    "\n",
    "for iu, unit in area_units.iterrows():\n",
    "    unit_spike_times = spike_times[iu]\n",
    "    \n",
    "    for j, stim in enumerate(stimuli):\n",
    "        presentations = presentations_sorted[presentations_sorted['image_name'] == stim]\n",
    "        num_presentations = len(presentations)\n",
    "        \n",
    "        for presentation_idx, presentation in presentations.iterrows():\n",
    "            start_time = presentations['start_time'][presentation_idx]\n",
    "            end_time = presentations['end_time'][presentation_idx]\n",
    "            duration = end_time - start_time\n",
    "            durations.append(duration)\n",
    "            \n",
    "            presentation_spike_times = unit_spike_times[(unit_spike_times >= start_time) * (unit_spike_times < end_time)]\n",
    "    \n",
    "            tuning_curves[unit_num, j] += len(presentation_spike_times) / duration / num_presentations\n",
    "    unit_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4506570",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(tuning_curves.T);\n",
    "plt.xlabel('Stimulus')\n",
    "plt.ylabel('Firing rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b01ae17",
   "metadata": {},
   "source": [
    "The signal correlation matrix is the correlation of neuron's trial-averaged responses---the similarity of their tuning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2678b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_correlations = np.corrcoef(tuning_curves)\n",
    "plt.imshow(signal_correlations, cmap='bwr', clim=(-1,1))\n",
    "plt.colorbar()\n",
    "plt.title('Signal Correlations')\n",
    "plt.xlabel('Unit #')\n",
    "plt.ylabel('Unit #')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ebd45c",
   "metadata": {},
   "source": [
    "Finally, let's compute the noise correlations. These are the correlations in the responses to each stimulus, reflecting the (correlated) trial-to-trial variability in the neural population. These correlations can come from synaptic connections (or indirect connections) between the neurons, so that when neuron A fires more on a given trial, neuron B also fires more (excitatory connection), or neuron B fires less (inhibitory connection). The noise correlations can also come from shared input. For example, if neuron C has an excitatory projection to both neurons A and B, then on trials where neuron C has increased firing rate, then both neurons A and B will also show increased firing.\n",
    "\n",
    "These noise correlations are defined on a per-stimulus basis and can vary somewhat between stimuli. For sake of interest, we'll plot below the correlation matrices for two different stimuli, and we'll later make use of the average correlation matrix (averaged over all 8 orientations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95108a22",
   "metadata": {},
   "source": [
    "Since noise correlations are single-trial correlations, if a neuron does not respond to a particular stimulus condition it can generate NaNs. To ignore these, we use numpy's masked array module, numpy.ma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a5949",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_correlations = np.zeros((len(conditions), num_units, num_units)) # initialize the noise correlation matrix for each stimulus condition\n",
    "\n",
    "for i, condition in enumerate(conditions):\n",
    "    condition_idx = np.where(labels == condition)\n",
    "    responses_condition = responses[condition_idx]\n",
    "    responses_condition = np.ma.masked_invalid(responses_condition)\n",
    "    \n",
    "    noise_correlations[i] = np.ma.corrcoef(responses_condition.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b88255",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_condition_idx = 0\n",
    "\n",
    "plt.imshow(noise_correlations[plot_condition_idx], cmap='bwr', clim=(-1,1))\n",
    "plt.colorbar()\n",
    "plt.title('Noise Correlations, {}'.format(conditions[plot_condition_idx]))\n",
    "plt.xlabel('Unit #')\n",
    "plt.ylabel('Unit #')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ee0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_condition_idx = -1\n",
    "\n",
    "plt.imshow(noise_correlations[plot_condition_idx], cmap='bwr', clim=(-1,1))\n",
    "plt.colorbar()\n",
    "plt.title('Noise Correlations, {}'.format(conditions[plot_condition_idx]))\n",
    "plt.xlabel('Unit #')\n",
    "plt.ylabel('Unit #')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38568c",
   "metadata": {},
   "source": [
    "Note that noise correlations can vary between stimuli! What differences do you see between these two noise correlation matrices?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc7e6d2",
   "metadata": {},
   "source": [
    "To get an overall view of the noise correlations, we average them across stimuli:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29168721",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_noise_correlations = np.mean(noise_correlations,axis=0)\n",
    "\n",
    "print('Mean noise correlation: {}'.format(np.mean(np.triu(mean_noise_correlations, 1))))\n",
    "\n",
    "plt.imshow(mean_noise_correlations, cmap='bwr', clim=(-1,1))\n",
    "plt.colorbar()\n",
    "plt.title('Noise Correlations, mean over stimuli')\n",
    "plt.xlabel('Unit #')\n",
    "plt.ylabel('Unit #')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459cebac",
   "metadata": {},
   "source": [
    "A common interpretation of noise correlations is that they can arise from synapses between, or common input to, a pair of neurons. These can also affect (or, through Hebbian learning, reflect) stimulus tuning. So we might expect noise and signal correlations to be correlated. Are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19efb95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncorr = np.matrix.flatten(mean_noise_correlations)\n",
    "diagonal_indices = np.diag_indices(num_units, ndim=2) \n",
    "flat_diagonal_indices = np.ravel_multi_index(diagonal_indices, dims=(num_units, num_units))\n",
    "ncorr = np.delete(ncorr, flat_diagonal_indices)\n",
    "\n",
    "print('Mean Noise Correlation:')\n",
    "print(np.mean(ncorr))\n",
    "\n",
    "scorr = np.matrix.flatten(signal_correlations)\n",
    "scorr = np.delete(scorr,flat_diagonal_indices)\n",
    "\n",
    "print('Mean Signal Correlation:')\n",
    "print(np.mean(scorr))\n",
    "\n",
    "plt.scatter(scorr,ncorr,s=1)\n",
    "plt.hlines(0,-1,1,colors='black')\n",
    "plt.vlines(0,-1,1,colors='black')\n",
    "plt.xlabel('Signal Correlation')\n",
    "plt.ylabel('Noise Correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8202a1",
   "metadata": {},
   "source": [
    "At first these look ~unrelated to each other! If we look closer, we'll note that there is a small (but statistically quite significant) trend for neuron pairs with higher signal correlation to have higher noise correlation, and vice versa.\n",
    "\n",
    "For this, we'll use the scipy pearsonr function instead of numpy corrcoef, because it returns a p-value from the hypothesis test where the null hypothesis is zero correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4815ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(ncorr,scorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be28c92",
   "metadata": {},
   "source": [
    "Various computational models make predictions about the relation between noise and signal correlations. For example, the local competition algorithm for sparse coding, which was once a leading theory of V1 computation, predicts a negative relationship between noise and signal correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6973a18e",
   "metadata": {},
   "source": [
    "## Population decoding with and without noise correlations\n",
    "\n",
    "While the noise correlations are weak, it is worth asking whether or not -- from an information processing standpoint -- we can treat each neuron as independent. In other words, are the noise correlations weak enough that they can be ignored?\n",
    "\n",
    "To test this, we'll return to our decoding analysis, and we will try decoding from synthetic data in which we artificially remove the noise correlations. We do this by trial-shuffling the neural data. This creates a fake dataset in which non-simultaneously-recorded neural activities are assembled to make the population response vectors, and it removes the noise correlations.\n",
    "\n",
    "To do this, we go through the data, and for each stimulus, and for each neuron, we randomly (and independently) re-order the trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bced5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_shuffle_responses(responses, conditions):\n",
    "    \n",
    "    shuffled_responses = responses.copy()\n",
    "\n",
    "    for i, condition in enumerate(conditions):\n",
    "        condition_idx = np.where(labels == condition)\n",
    "\n",
    "        for j in range(num_units):\n",
    "            responses_unit_condition = responses[condition_idx, j].reshape(-1).copy()\n",
    "            np.random.shuffle(responses_unit_condition) # shuffle in place\n",
    "            shuffled_responses[condition_idx, j] = responses_unit_condition\n",
    "            \n",
    "    return shuffled_responses\n",
    "\n",
    "shuffled_responses = trial_shuffle_responses(responses, conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17663d",
   "metadata": {},
   "source": [
    "First, let's double-check that our shuffling worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff64fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_noise_correlations = np.zeros((len(conditions), num_units, num_units)) # initialize the noise correlation matrix for each stimulus condition\n",
    "\n",
    "for i, condition in enumerate(conditions):\n",
    "    condition_idx = np.where(labels == condition)\n",
    "    responses_condition = shuffled_responses[condition_idx]\n",
    "    responses_condition = np.ma.masked_invalid(responses_condition)\n",
    "    \n",
    "    shuffled_noise_correlations[i] = np.ma.corrcoef(responses_condition.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f0c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_shuffled_noise_correlations = np.mean(shuffled_noise_correlations,axis=0)\n",
    "\n",
    "plt.imshow(mean_shuffled_noise_correlations, cmap='bwr', clim=(-1,1))\n",
    "plt.colorbar()\n",
    "plt.title('Noise Correlations (shuffled trials), mean over stimuli')\n",
    "plt.xlabel('Unit #')\n",
    "plt.ylabel('Unit #')\n",
    "\n",
    "print('Mean noise correlation after trial shuffling: {}'.format(np.mean(np.triu(mean_shuffled_noise_correlations, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5319451",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins=np.arange(-.2, .2, .01)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(np.triu(mean_shuffled_noise_correlations, 1).reshape(-1,), bins=bins, histtype='step', linewidth=2, label='Shuffled')\n",
    "plt.hist(np.triu(mean_noise_correlations, 1).reshape(-1,), bins=bins, histtype='step', linewidth=2, label='Original')\n",
    "plt.legend(loc=0, frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c06e353",
   "metadata": {},
   "source": [
    "We can see that while the noise correlations in the original data are small, they exhibit a positive tail absent in the shuffled data. Now let's decode from the trial-shuffled responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff1b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_shuffled = []\n",
    "confusions_shuffled = []\n",
    "\n",
    "for train_indices, test_indices in KFold(n_splits=num_splits, shuffle=True).split(responses):\n",
    "\n",
    "    clf = svm.SVC()\n",
    "\n",
    "    clf.fit(shuffled_responses[train_indices], labels[train_indices])\n",
    "    \n",
    "    test_targets = labels[test_indices]\n",
    "    test_predictions = clf.predict(shuffled_responses[test_indices])\n",
    "    \n",
    "    accuracy = np.mean(test_targets == test_predictions)    \n",
    "    print(accuracy)\n",
    "    \n",
    "    accuracies_shuffled.append(accuracy)\n",
    "    confusions_shuffled.append(confusion_matrix(y_true=test_targets, y_pred=test_predictions, labels=conditions, normalize='pred'))\n",
    "    \n",
    "print(f\"\\nmean accuracy, shuffled trials: {np.mean(accuracies_shuffled)}\")\n",
    "print(f\"chance: {1/conditions.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de54b31",
   "metadata": {},
   "source": [
    "# With these analyses in hand, we leave you with some questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee63c00a",
   "metadata": {},
   "source": [
    "### If you integrate spikes in a fixed window length, how does the decoding accuracy depend on the time since the image presentation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4251ff51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8859d9a4",
   "metadata": {},
   "source": [
    "### Do noise correlations impact the decoding on specific timescales or for specific population sizes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b0c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### timescales (fixed population size)\n",
    "\n",
    "window_lengths = np.arange(.01, .1, .02)\n",
    "\n",
    "accuracies = np.zeros((len(window_lengths), num_splits))\n",
    "shuffled_accuracies = np.zeros((len(window_lengths), num_splits))\n",
    "\n",
    "for i, window in enumerate(window_lengths):\n",
    "    print('{}/{}'.format(i, len(window_lengths)))\n",
    "    responses, labels = make_response_array(spike_times, stimulus_presentations, area_units, window)\n",
    "    shuffled_responses = trial_shuffle_responses(responses, conditions)\n",
    "    \n",
    "    k = 0\n",
    "    for train_indices, test_indices in KFold(n_splits=num_splits, shuffle=True).split(responses):\n",
    "        clf = svm.SVC()\n",
    "        clf.fit(responses[train_indices], labels[train_indices])\n",
    "\n",
    "        test_targets = labels[test_indices]\n",
    "        test_predictions = clf.predict(responses[test_indices])\n",
    "\n",
    "        accuracies[i, k] = np.mean(test_targets == test_predictions)        \n",
    "        \n",
    "        clf.fit(shuffled_responses[train_indices], labels[train_indices])\n",
    "\n",
    "        test_targets = labels[test_indices]\n",
    "        test_predictions = clf.predict(shuffled_responses[test_indices])\n",
    "\n",
    "        shuffled_accuracies[i, k] = np.mean(test_targets == test_predictions) \n",
    "              \n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b96ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.errorbar(x=window_lengths, y=accuracies.mean(axis=(1)), yerr=accuracies.std(axis=(1)) / np.sqrt(num_splits), fmt='-')\n",
    "plt.errorbar(x=window_lengths, y=shuffled_accuracies.mean(axis=(1)), yerr=accuracies_shuffled_corrs.std(axis=(1))/ np.sqrt(num_splits), fmt='-')\n",
    "\n",
    "plt.xlabel('Population size')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### population sizes (fixed timescale)\n",
    "\n",
    "window = .2\n",
    "\n",
    "responses, labels = make_response_array(spike_times, stimulus_presentations, area_units, window)\n",
    "shuffled_responses = trial_shuffle_responses(responses, conditions)\n",
    "\n",
    "accuracies = np.zeros((len(window_lengths), num_resamples, num_splits))\n",
    "shuffled_accuracies = np.zeros((len(window_lengths), num_splits))\n",
    "\n",
    "for i, pop_size in enumerate(pop_sizes):\n",
    "    print('population size: {}'.format(pop_size))\n",
    "    for j in range(num_resamples):\n",
    "        pop_idx = np.random.choice(range(num_units), size=pop_size)\n",
    "        responses_pop = responses[:, pop_idx]\n",
    "        shuffled_responses_pop = shuffled_responses[:, pop_idx]\n",
    "        \n",
    "        k = 0\n",
    "        for train_indices, test_indices in KFold(n_splits=num_splits, shuffle=True).split(responses_pop):\n",
    "            clf = svm.SVC()\n",
    "            clf.fit(responses_pop[train_indices], labels[train_indices])\n",
    "\n",
    "            test_targets = labels[test_indices]\n",
    "            test_predictions = clf.predict(responses_pop[test_indices])\n",
    "\n",
    "            accuracies[i, j, k] = np.mean(test_targets == test_predictions)        \n",
    "\n",
    "            clf.fit(shuffled_responses_pop[train_indices], labels[train_indices])\n",
    "\n",
    "            test_targets = labels[test_indices]\n",
    "            test_predictions = clf.predict(shuffled_responses_pop[test_indices])\n",
    "\n",
    "            shuffled_accuracies[i, j, k] = np.mean(test_targets == test_predictions) \n",
    "\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e602960",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.errorbar(x=pop_sizes, y=accuracies.mean(axis=(1, 2)), yerr=accuracies.std(axis=(1, 2)) / np.sqrt(num_resamples*num_splits), fmt='-')\n",
    "plt.errorbar(x=pop_sizes, y=accuracies_shuffled_corrs.mean(axis=(1, 2)), yerr=accuracies_shuffled_corrs.std(axis=(1, 2))/ np.sqrt(num_resamples*num_splits), fmt='-')\n",
    "\n",
    "plt.xlabel('Population size')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b0d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "167e64c4",
   "metadata": {},
   "source": [
    "### Are the accuracy curves different for familiar vs novel images?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce64c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e511b6cf",
   "metadata": {},
   "source": [
    "### Are the accuracy curves different in active vs passive blocks? Do noise correlations have differential impacts in those blocks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f0b507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "278570a3",
   "metadata": {},
   "source": [
    "### Is the structure of the population code different on hit vs miss trials? Can you predict hit vs miss by comparing the lick time to the decoding time course?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269244a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a628ae2",
   "metadata": {},
   "source": [
    "### Are other variables, including behavioral variables, also encoded in the population activity? Can you decode the running speed, pupil diameter, or licking behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd78c801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d885d65d",
   "metadata": {},
   "source": [
    "### What about in a different brain area? For example, is the image encoded in CA1 activity? What about in the joint activity across brain areas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c273d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swdb",
   "language": "python",
   "name": "swdb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
